ChatGPT был доработан поверх GPT-3.5 с использованием методов обучени€ как с учителем, так и с подкреплением. ¬ обоих подходах использовались люди-тренеры дл€ улучшени€ производительности модели. ¬ случае обучени€ с учителем модель была снабжена беседами, в которых тренеры играли обе стороны: пользовател€ и помощника по искусственному интеллекту. Ќа этапе подкреплени€ инструкторы-люди сначала оценивали ответы, которые модель создала в предыдущем разговоре. Ёти оценки были использованы дл€ создани€ моделей вознаграждени€, на которых модель была дополнительно доработана с использованием нескольких итераций Proximal Policy Optimization[19][20]. јлгоритмы Proximal Policy Optimization имеют преимущество по затратам по сравнению с алгоритмами Region Policy Optimization; они свод€т на нет многие дорогосто€щие в вычислительном отношении операции с более высокой производительностью[21][22]. ћодели были обучены в сотрудничестве с Microsoft на их суперкомпьютерной инфраструктуре Azure.