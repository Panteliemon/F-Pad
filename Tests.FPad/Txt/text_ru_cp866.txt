ChatGPT был доработан поверх GPT-3.5 с использованием методов обучения как с учителем, так и с подкреплением. В обоих подходах использовались люди-тренеры для улучшения производительности модели. В случае обучения с учителем модель была снабжена беседами, в которых тренеры играли обе стороны: пользователя и помощника по искусственному интеллекту. На этапе подкрепления инструкторы-люди сначала оценивали ответы, которые модель создала в предыдущем разговоре. Эти оценки были использованы для создания моделей вознаграждения, на которых модель была дополнительно доработана с использованием нескольких итераций Proximal Policy Optimization[19][20]. Алгоритмы Proximal Policy Optimization имеют преимущество по затратам по сравнению с алгоритмами Region Policy Optimization; они сводят на нет многие дорогостоящие в вычислительном отношении операции с более высокой производительностью[21][22]. Модели были обучены в сотрудничестве с Microsoft на их суперкомпьютерной инфраструктуре Azure.